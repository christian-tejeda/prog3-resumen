# Algoritmos Ávidos

* Decimos que un algoritmo es ávido si construye una solución en pasos pequeños, decidiendo en cada paso para optimizar algún criterio subyacente.
* Esto nos dice algo útil sobre la estructura del problema en sí, ya que hay una regla de decisión local sobre la cual construir soluciones óptimas.
* Dos métodos básicos para demostrar la optimalidad de una solución producida por un algoritmo ávido:
* El primero es establecer que el algoritmo se mantiene a la cabeza (_stays ahead_). Es decir que al analizar el progreso del algoritmo paso a paso, es mejor que cualquier otro algoritmo en cada paso.
* El segundo es un argumento de intercambio (_exchange argument_): se considera una solución posible y se transforma gradualmente en la solución encontrada por el algoritmo ávido sin dañar su calidad.

### Planificación de intervalos: el algoritmo ávido _stays ahead_

##### Diseñando un algoritmo ávido

* Problema: planificación de intervalos (_interval scheduling_)
	* Conjunto de $\{1, 2, \dots, n\}$ _pedidos_. El $i$-ésimo pedido corresponde a un _intervalo_ de tiempo comenzando en $s(i)$ y finalizando en $f(i)$.
	* Dos pedidos son _compatibles_ si sus intervalos de tiempo no se superponen.
	* Buscamos el máximo conjunto de pedidos mutuamente compatibles.
	* Una regla ávida que conduce a la solución óptima es aceptar primero el pedido $i$ para el cual $f(i)$ sea lo más pequeño posible. Nos aseguramos de que el recurso se libera tan pronto como sea posible mientras completa el pedido.

* Algoritmo: planificación de intervalos.
	* ![[intsched.png]]

##### Analizando el algoritmo

* Propiedad: conjunto ávido es compatible (4.1)
	* $A$ es un conjunto de pedidos compatibles.

* Primero notamos que los intervalos en el conjunto $A$ devuelto por el algoritmo son todos compatibles entre sí.
* Debemos mostrar que esta solución es óptima. Sea $\mathcal{O}$ una solución óptima de intervalos. Mostraremos simplemente que $|A| = |\mathcal{O}|$, es decir que $A$ contiene la misma cantidad de intervalos que $\mathcal{O}$ y por lo tanto es una solución óptima.
* Sea $i_{1}, \dots, i_{k}$ el conjunto de pedidos en $A$ en el orden en que fueron añadidos a $A$, $|A| = k$. Sea $j_{1}, \dots, j_{m}$ el conjunto de pedidos de $\mathcal{O}$. El objetivo es demostrar que $k = m$.
* Nuestra regla ávida garantiza que $f(i_{1}) \leq f(j_{1})$. Este el sentido en el que mostramos que el algoritmo ávido se mantiene a la cabeza: cada uno de los intervalos termina como mínimo tan pronto como los intervalos correspondientes en el conjunto $\mathcal{O}$. Se extiende el argumento para cada $r \geq 1$: el $r$-ésimo pedido aceptado en el algoritmo termina no más tarde que la $r$-ésimo pedido en la planificación óptima.

* Lema: regla ávida de planificación (4.2)
	* Para todos los índices $r < k$, se tiene que $f(i_{r}) \leq f(j_{r})$.

* Propiedad: optimalidad del conjunto ávido (4.3)
	* El algoritmo ávido devuelve un conjunto óptimo $A$.

* Se puede implementar el algoritmo para que corra en tiempo $O(n \log n)$.
* Se ordenan los $n$ pedidos según el tiempo de finalización, esto cuesta $O(n \log n)$.
* Se mantiene un arreglo $S[i]$ que contiene el valor $s(i)$ en tiempo $O(n)$.
* Si el intervalo más reciente agregado termina en el momento $f$, iteramos sobre los subsiguientes intervalos hasta encontrar el primer $j$ tal que $s(j) > f$. Esto cuesta tiempo $O(n)$ sobre los intervalos.

### Partición de intervalos (_interval partitioning_)

* En vez de disponer de un solo recurso como en la planificación, disponemos de varios recursos idénticos y queremos planificar todos los pedidos usando la menor cantidad de recursos posible.
* La _profundidad_ de un conjunto de intervalos es la máxima cantidad de pedidos que pasan al mismo tiempo.

* Propiedad: cantidad de recursos necesarios (4.4)
	* En cualquier instancia de _Interval Partitioning_, el número de recursos necesario es como mínimo la profundidad del conjunto de intervalos.

* Sea $d$ la profundidad del conjunto de intervalos: mostramos cómo asignar una _etiqueta_ del conjunto $\{1, 2, \dots, d\}$ a cada intervalo. La asignación tiene la propiedad de que los intervalos superpuestos están etiquetados con números diferentes.
* El algoritmo ávido ordena los intervalos por su tiempo de comienzo. Recorremos los intervalos en este orden e intentamos asignar a cada intervalo una etiqueta que no haya sido asignada aún a cualquier intervalo superpuesto anterior.

* Algoritmo: partición de intervalos ávida
	* ![[intervalscheduling.png]]

* Propiedad: regla ávida no asigna misma etiqueta a intervalos superpuestos (4.5)
	* Utilizando el algoritmo ávido descrito, cada intervalo será asignado a una etiqueta y ningún par de intervalos que se sobrepongan recibirán la misma etiqueta.

* Esencialmente, si tenemos $d$ etiquetas a disposición, a medida que se barren los intervalos de izquierda a derecha asignando una etiqueta disponible a cada intervalo encontrado, nunca se alcanza un punto donde todas las etiquetas estén en uso.
* Como nuestro algoritmo usa $d$ etiquetas, podemos usar (4.4) para concluir que, en efecto, siempre se usa el mínimo de etiquetas posible.

* Propiedad: optimalidad de partición ávida (4.6)
	* El algoritmo ávido planifica cada intervalo a un recurso, usando una cantidad igual a la profundidad del conjunto de intervalos. Este es el número óptimo de recursos necesarios.

### Planificando para minimizar retrasos: argumento de intercambio

##### El problema

* Consideramos la situación donde tenemos un solo recurso y un conjunto de $n$ pedidos para usar el recurso por un intervalo de tiempo. El recurso está disponible desde el momento $s$. En vez de un tiempo de comienzo y finalización, el pedido $i$ tiene un _plazo_ $d_{i}$ y requiere un intervalo de tiempo contiguo de largo $t_{i}$. Se puede planificar en cualquier momento previo al plazo.
* Cada pedido aceptado debe tener asignado un intervalo de tiempo de largo $t_{i}$ y pedidos distintos deben tener asignados intervalos que no se superponen.

* Planeamos satisfacer cada pedido, pero solo tenemos permitido dejar que ciertos pedidos se retrasen. Al comienzo en el moento $s$, asignamos cada pedido $i$ a un intervalo de tiempo $t_{i}$: sea el intervalo $[s(i), f(i)]$, con $f(i) = s(i) + t_{i}$. El algoritmo debe determinar un tiempo de comienzo y finalización para cada intervalo.
* Decimos que un pedido $i$ está _retrasado_ si no cumple el plazo: $f(i) > d$. El _retraso_ de un pedido se define como $l_{i} = f(i) - d_{i}$, si $l_{i} = 0$ decimos que el pedido $i$ no tiene retraso.
* El objetivo en el problema de optimización será asignar todos los pedidos, usando intervalos no superpuestos, de forma que se minimize el _máximo retraso_: $L = \max_{i} l_{i}$.

##### Diseñando el algoritmo

* Ordenamos los pedidos en orden creciente según el plazo $d_{i}$ y planificamos según este orden. Esta regla es llamada _earliest deadline first_. La intuición dice que los pedidos con plazos tempranos deben completarse antes.

* Algoritmo: earliest deadline first
	* ![[schedminlateness1.png]] ![[schedminlateness2.png]]

##### Analizando el algoritmo

* Para establecer la optimalidad del algoritmo, primero observamos que la planificación producida no tiene huecos. El tiempo que ocurre durante un hueco se llama _tiempo muerto_. 
* La planificación $A$ producida por el algoritmo no tiene tiempo muerto.

* Lema: existencia de planificación óptima (4.7)
	* Existe una planificación óptima sin tiempo muerto.

* Definición: inversión.
	* Dada una planificación $S$, una inversión es un par de pedidos $i$ y $j$ tal que $i < j$ pero $j$ está asignado antes que $i$.

* Propiedad: correctitud del algoritmo (4.7, 4.8, 4.9 y 4.10).
	* Todas las planificaciones sin inversiones ni tiempo muerto tienen el mismo máximo retraso.
		* El máximo retraso no depende del orden de asignación.
	* Existe una planificación óptima sin inversiones ni tiempo muerto.
		* Intercambiamos las inversiones de una solución posiblemente óptima hasta que no las tenga, manteniendo el máximo retraso igual.
	* La planificación $S$ producida por el algoritmo ávido tiene máximo retraso $L$ óptimo.


### Caminos más cortos en un grafo

* Problema:
	* Tenemos un grafo dirigido $G = (V, E)$ con un nodo designado para comenzar $s$. Asumimos que $s$ tiene un camino a cualquier otro nodo de $G$. Cada arista $e$ tiene una distancia o costo $l_{e} \geq 0$ para atravesarlo. Para un camino $P$, el largo $l(P)$ es la suma de las distancias de todas las aristas de $P$.
	* Nuestro objetivo es determinar el _camino más corto_ desde $s$ a cualquier otro nodo en el grafo.

##### Diseñando el algoritmo

* Mantenemos un conjunto $S$ de nodos $u$ para los cuales hemos determinado el camino más corto de distancia $d(u)$ desde $s$.
* Inicialmente $S = {s}$ y $d(s) = 0$.
* Ahora para cada nodo $v \in V - S$ consideramos la cantidad $d'(v) = \min_{e = (u, v): u \in S} \ d(u) + l_{e}$ y elegimos el nodo que minimiza este valor.

* Algoritmo de Dijkstra.
	* ![[dijkstra.png]]
	* ![[slide_dijkstra.png]]

* Propiedad: caminos más cortos (4.14).
	* Considerar el conjunto $S$ en cualquier momento de la ejecución del algoritmo. Para cada $u \in S$, el camino $P_{u}$ es el camino $s-u$ más corto.

* Propiedad: tiempo de ejecución de Dijkstra (4.15).
	* Usando una cola de prioridad, el algoritmo de Dijkstra puede ser implementado en un grafo de $n$ nodos y $m$ aristas para correr en tiempo $O(m)$ más el tiempo para $n$ operaciones de extracción de clave y $m$ operaciones de cambio de clave. Cada operación de la cola de prioridad toma tiempo $O(\log n)$, por lo que el tiempo total de la implementación es $O(m \log n)$.

### Árboles de recubrimiento mínimo

* Problema:
	* Tenemos un grafo $G = (V, E)$ con un costo $c_{e}$ asociado a cada arista $e = (v_{i}, v_{j})$. El problema consiste en encontrar un subconjunto de aristas $T \subseteq E$ tal que el grafo $(V, T)$ sea conexo, y el costo total $\sum_{e \in T} c_{e}$ sea lo más pequeño posible.

* Lema: (4.16)
	* Sea $T$ una solución de coste mínimo al problema. Entonces $T$ es un árbol.
	* Si no fuera un árbol, tendría un ciclo. Quitando una arista $e$ de este ciclo obtenemos una solución $T - {e}$ menos costosa, pero esto contradice la optimalidad de la solución $T$.

* Definición: árbol recubridor.
	* $T \subseteq E$ es un _árbol recubridor_ de $G$ si $(V, T)$ es un árbol.

* Buscamos el árbol recubridor que tenga el _mínimo coste_ posible.
* Planteamos tres algoritmos ávidos correctos:
	* Algoritmo de Kruskal: comenzamos sin ninguna arista y se construye un árbol recubridor insertando aristas de $E$ en orden de coste creciente, evitando formar ciclos. Si la arista $e$ resulta en un ciclo, entonces se descarta.
	* Algoritmo de Prim: comenzamos desde un nodo inicial $s$ y se intenta hacer crecer con avidez un árbol $S$. En cada paso, agregamos el nodo $v$ que minimiza el "coste de fijación" $\min_{e=(u, v): u \in S} c_e$ e incluyendo la arista $e$ que alcanza este mínimo en el árbol recubridor.
	* Algoritmo Reverse-Delete: Comenzamos con el grafo $(V, E)$ y borramos aristas en orden de coste decreciente de forma que el grafo no quede desconectado.

* Propiedad: cut property (4.17)
	* Asumir que todas las aristas tienen coste distinto. Sea $S$ cualquier subconjunto de nodos que no es vacío ni igual a $V$ y sea $e = (v, w)$ la arista de coste mínimo con un extremo en $S$ y otro en $V - S$. Entonces todo árbol de recubrimiento mínimo contiene la arista $e$.

* Propiedad: cycle property (4.20)
	* Asumir que todas las aristas tienen coste distinto. Sea $C$ cualquier ciclo en $G$ y sea $e = (v, w)$ la arista más costosa perteneciente a $C$. Entonces $e$ no pertenece a ningún árbol de recubrimiento mínimo de $G$.

* Propiedad: correctitud de los algoritmos (4.18, 4.19, 4.21)
	* El algoritmo de Kruskal produce un árbol de recubrimiento mínimo de $G$.
	* El algoritmo de Prim produce un árbol de recubrimiento mínimo de $G$.
	* El algoritmo Reverse-Delete produce un árbol de recubrimiento mínimo de $G$.
	* Las aristas agregadas por Kruskal/Prim son las menos costosas según la propiedad de corte. Las aristas eliminadas por Reverse-Delete son las más costosas según la propiedad del ciclo.

* Propiedad: eliminar la suposición de costes distintos en las aristas.
	* Se perturban los costos de las aristas con números extremadamente pequeños para que se vuelvan distintos. El orden relativo se preserva al ser la perturbación tan pequeña.
	* Cualquier árbol $T$ para una instancia perturbada también es un MST de la instancia original.

### Implementando el algoritmo de Prim

* Propiedad: tiempo de ejecución de Prim (4.22)
	* Usando una cola de prioridad, el algoritmo de Prim puede ser implementado en un grafo con $n$ nodos y $m$ aristas para ejecutarse en tiempo $O(m)$ más el tiempo para $n$ operaciones $\texttt{ExtractMin}$ y $m$ operaciones $\texttt{ChangeKey}$.
	* Usando una cola de prioridad basda en montículos, las operaciones $\texttt{ExtractMin}$ y $\texttt{ChangeKey}$ se pueden implementar en tiempo $O(\log n)$, para obtener un tiempo total de $O(m \log n)$.

* Algoritmo:
	* ![[slide_prim.png]]

### Implementando el algoritmo de Kruskal: la estructura de datos Union-Find

* Problema:
	* La estructura $\texttt{Union-Find}$ nos permite mantener conjuntos disjuntos, tal como los componentes de un grafo $G$ al que se le agregan aristas.
	* Dado un nodo $u$, la operación $\texttt{Find}(u)$ retorna el nombre del conjunto que contiene a $u$. Esta operación se puede usar para comprobar si dos nodos $u$ y $v$ están en la misma componente, chequeando si $\texttt{Find}(u) = \texttt{Find}(v)$.
	* La estructura de datos también implementa una operación $\texttt{Union}(A, B)$ que tome dos conjuntos $A$ y $B$ y los una en uno solo.
	* Si agregamos una arista $(u, v)$ al grafo, primero comprobamos si $u$ y $v$ ya están en la misma componente conexa. Si no lo están, entonces se unen los dos componentes con $\texttt{Union}(\texttt{Find}(u), \texttt{Find}(v))$.
	* $\texttt{MakeUnionFind}(S)$ retornará una estructura de datos $\texttt{Union-Find}$ sobre el conjunto $S$ donde todos los elementos están en conjuntos separados, que corresponde a las componentes conexas de un grafo sin aristas.

##### Una estructura de datos mejor para Union-Find

* Cada nodo $v \in S$ es contenido en un registro con un puntero asociado al nombre del conjunto que lo contiene.
* $\texttt{MakeUnionFind}(S)$ se inicializa un registro para cada elemento de $S$ con un puntero a sí mismo o uno nulo para indicar que $v$ está en su propio conjunto.
* La operación $\texttt{Union}$ para dos conjuntos $A$ y $B$ representados por $v$ y $u$ actualiza el puntero de $u$ para apuntar a $v$. No se actualizan los demás nodos del conjunto $B$.
* El nombre del conjunto al que pertenece un nodo se obtiene siguiendo una secuencia de punteros.

![[figura4_12.png]]

* Propiedad: tiempos de implementación basada en punteros (4.24).
	* Para la implementación basada en punteros descrita de la estructura $\texttt{Union-Find}$ para un conjunto $S$ de tamaño $n$ donde las uniones mantienen el nombre del conjunto más grande, $\texttt{Union}$ toma tiempo $O(1)$, $\texttt{MakeUnionFind}(S)$ toma tiempo $O(n)$ y $\texttt{Find}$ toma tiempo $O(\log n)$.

##### Más mejoras

* Un caso malo donde cada operación $\texttt{Find}$ toma tiempo $O(\log n)$ se obtiene tomando uniones de conjuntos del mismo tamaño.
* Sea $v$ un nodo para el cual $\texttt{Find}(v)$ toma tiempo $O(\log n)$. Para cada llamado subsecuente la operación toma el mismo tiempo $\log n$, aunque ya sabemos a qué conjunto pertenece desde el primer llamado, junto con los nodos siguientes desde $v$.
* Para mejorar la implementación comprimiremos el camino después de cada operación $\texttt{Find}$ reiniciando todos los punteros a lo largo del camino para apuntar al nombre del conjunto actual.

![[figura4_13.png]]

* Propiedad: tiempos de ejecución con compresión de caminos.
	* Para la implementación basada en punteros con compresión de caminos $\texttt{Union}$ toma tiempo $O(1)$, $\texttt{MakeUnionFind}(S)$ toma tiempo $O(n)$ y $\texttt{Find}$ toma tiempo $O(\log n)$ en el peor caso. Una secuencia de $n$ llamados a $\texttt{Find}$ toma tiempo $O(n \cdot \alpha(n))$, donde $\alpha(n)$ es la función de Ackermann inversa ($\alpha(n) \leq 4$ para todo valor práctico de $n$).

##### Implementando el algoritmo de Kruskal

* Primero se ordena las aristas por su coste. Esto toma tiempo $O(m \log m)$. Al tener al menos una arista entre cualquier par de nodos, tenemos $m \leq n^2$ por lo que el tiempo es también $O(m \log n)$.
* Luego se usa la estructura de datos $\texttt{Union-Find}$ para mantener las componentes conexas de $(V, T)$ a medida que se agregan aristas. Cuando se considera una arista $e = (u, v)$ se calcula $\texttt{Find}(u)$ y $\texttt{Find}(v)$ para ver si pertenecen a componentes distintas.
* Se usa $\texttt{Union}(\texttt{Find}(u), \texttt{Find}(v))$ para unir ambos componentes si el algoritmo decide incluir la arista $e$ en el árbol $T$.
* Con la implementación basada en (4.23, 4.24) el tiempo total es $O(m \log n)$.

* Algoritmo: algoritmo de Kruskal.
	* ![[slide_kruskal.png]]

* Propiedad: tiempo de ejecución de Kruskal (4.25).
	* El algoritmo de Kruskal puede ser implementado en un grafo de $n$ nodos y $m$ aristas para ejecutarse en tiempo $O(m \log n)$.