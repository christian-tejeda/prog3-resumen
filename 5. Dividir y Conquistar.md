# Dividir y Conquistar

* _Dividir y conquistar_ refiere a una clase de técnicas algorítmicas en las cual se descompone la entrada en varias partes, resuelve el problema en cada parte recursivamente y combina las soluciones de esos subproblemas en una solución general.
* Analizar el tiempo de ejecución de un algoritmo dividir y conquistar generalmente involucra resolver una _relación de recurrencia_ que acota el tiempo de ejecución recursivamente en términos del tiempo en instancias más pequeñas.

### Una primer recurrencia: el algoritmo Mergesort

* El algoritmo Mergesort ordena una lista de números dada dividiéndola en dos mitades iguales, ordenando cada una recursivamente y combinando los resultados de esos llamados recursivos utilizando el algoritmo de tiempo lineal para unir dos listas ordenadas (del cap. 2).

* Algoritmo: 
	* ![[slide_mergesort.png|600]]

* Sea $T(n)$ el tiempo de ejecución en el peor caso para un algoritmo DyV. 
	* Si $n$ es par, el algoritmo toma tiempo $O(n)$ para dividir la entrada en dos partes iguales de tamaño $\frac{n}{2}$.
	* Resolver cada parte toma $T(\frac{n}{2})$ en el peor caso.
	* Finalmente toma tiempo $O(n)$ para combinar las soluciones de los dos llamados recursivos.

* Lema: cota para el tiempo de ejecución de Mergesort (5.1)
	* $T(n)$ satisface la siguiente relación de recurrencia: para alguna constante $c$, $T(n) \leq 2 T(\frac{n}{2}) + cn$ cuando $n > 2$ y $T(2) \leq c$.

* Hay dos formas básicas de resolver una recurrencia:
	* La forma más intuitiva es "desenrollar" la recursión, analizando los primeros niveles e identificando un patrón a medida que la recursión se expande. Luego se suman los tiempos en todos los niveles de la recursión.
	* Comenzar con una supuesta solución, substituirla en la relación de recurrencia y chequear si la cumple.

##### Resolviendo la recurrencia de Mergesort

* En el primer nivel de la recursión, tenemos un solo problema de tamaño $n$, que toma a lo sumo tiempo $cn$ más el tiempo en los llamados recursivos subsecuentes. En el siguiente nivel, tenemos dos problemas de tamaño $\frac{n}{2}$, cada uno toma a lo sumo tiempo $\frac{cn}{2}$ para un total de $cn$ más el tiempo de los llamados recursivos siguientes. En el tercer nivel tenemos cuatro problemas de tamaño $\frac{n}{4}$, cada uno toma tiempo $\frac{cn}{4}$ para un total de $cn$.
* En el nivel $j$ de la recursión, la cantidad de subproblemas se ha duplicado $j$ veces, por lo que hay un total de $2^{j}$ problemas de tamaño $\frac{n}{2^{j}}$. Cada problema toma a lo sumo tiempo $\frac{cn}{2^{j}}$. El nivel $j$ contribuye un total de $2^{j} \frac{cn}{2^{j}} = cn$ al tiempo total.
* La cantidad de veces que la entrada debe ser dividida para reducir su tamaño de $n$ a $2$ es $\log_{2} n$. Sumando el tiempo $cn$ sobre los $\log n$ niveles de recursión obtenemos un tiempo total de ejecución de $O(n \log n)$.

* Propiedad: recurrencia de Mergesort (5.2).
	* Cualquier función $T(\cdot)$ que satisfaga la relación de recurrencia descrita está acotada por $O(n \log n)$ cuando $n > 1$.

##### Sustituyendo una solución en la recurrencia de Mergesort

* El argumento en (5.2) se puede usar para determinar que la función $T(n)$ está acotada por $O(n \log n)$.
* Si en su lugar tenemos una suposición para verificar el tiempo de ejecución, lo hacemos sustituyendola en la recurrencia.
* Suponemos que $T(n) \leq cn \log_{2} n, \forall n \geq 2$ y queremos verificar si esto es cierto por inducción.
	* Es cierto para $n = 2$ por (5.1).
	* Asumimos que se cumple la inecuación para todo $m < n$. Sustituyendo en la recurrencia: 
	* $$\begin{align}
       T(n) \leq & \ 2T(n/2) + cn \\
       \leq & \ 2c(n/2) \log_{2}(n/2) + cn \\
       = & \ cn(\log_{2} (n) -1) + cn \\
       = & \ (cn \log_{2} (n) - 1) - cn + cn \\
       = & \ cn \log_{2}(n)
       \end{align}
       $$
	* Esto establece la cota para $T(n)$, completando el argumento por inducción.

##### Un enfoque usando sustitución parcial

* Hay una sustitución más débil que se puede hacer, en la cuál se supone una forma general de la solución sin encontrar los valores exactos de las constantes.
* Específicamente suponemos que $T(n) = O(n \log n)$ pero no conocemos la constante de $O(\cdot)$. Utilizamos el método de sustitución como sigue:
* Primero tenemos que $T(n) \leq k n \log_{b} n$ para alguna constante $k$ y base $b$. Nos preguntamos si hay una elección de $k$ y $b$ que funcione en un argumento inductivo.
	* $T(n) \leq 2 T(n/2) + cn \leq 2k(n/2) \log_{2} n/2 + cn$. Elegimos la base $b = 2$ para simplificar el logaritmo:
	* $$\begin{align}
       T(n) \leq & \ 2k(n/2) \log_{2}(n/2) + cn \\
       = & \ 2k(n/2)(\log_{2} (n) - 1) + cn \\
       = & \ kn (\log_{2} (n) - 1) + cn \\
       = & \ kn \log_{2} (n) - kn + cn
       
       \end{align}
       $$
	* Elegimos un $k$ tan grande como $c$ para obtener $T(n) \leq kn \log_{2} n - kn + cn \leq kn \log_{2} n$
	* Esto completa el argumento inductivo.

### Más relaciones de recurrencia

* Consideramos una clase de relaciones de recurrencias que generaliza a (5.1). Esta clase de obtiene considerando algoritmos dividir y conquistar que crean llamados recursivos sobre $q$ subproblemas de tamaño $n/2$ y combina los resultados en tiempo $O(n)$.
* Propiedad: si $T(n)$ denota el tiempo de ejecución de un algoritmo de esta clase, entonces $T(n)$ satisface la siguiente relación de recurrencia: (5.3)
	* Para alguna constante $c$, $T(n) \leq q T(n/2) + cn$ cuando $n > 2$, y 
	* $T(2) \leq c$.

##### El caso de $q > 2$ subproblemas

* Se analiza cada nivel para el caso $q = 3$ como en la figura.
* En un nivel arbitrario $j$, tenemos $q^{j}$ instancias distintas de tamaño $n/2^{j}$. El tiempo total en el nivel $j$ es $(q/2)^{j} cn$. ![[figura5_2.png|500]]
* Se suman todos los niveles de la recursión. ![[mergesort_geosum.png]]

* Propiedad: caso general de la recurrencia (5.4)
	* Cualquier función $T(\cdot)$ que satisfaga (5.3) con $q > 2$ está acotada por $O(n^{\log_{2} q})$.

##### El caso de un solo subproblema

* Se analiza la recurrencia para el caso $q = 1$ como en la figura: ![[figura5_3.png]]
* En el nivel $j$ tenemos una sola instancia de tamaño $n/2^{j}$ que contribuye $cn/2^{j}$ al tiempo total. En total hay $\log_{2} n$ niveles de recursión.
* El tiempo total es la suma de todos los niveles de recursión, lo que da como resultado $T(n) \leq 2cn = O(n)$.

* Propiedad: caso $q = 1$ (5.5)
	* Cualquier función $T(\cdot)$ que satisfaga (5.3) con $q = 1$ está acotada por $O(n)$.

##### Una recurrencia relacionada: $T(n) \leq 2 T(n/2) + O(n^2)$

* La recurrencia está basada en la siguiente estructura:
	* Divide la entrada en dos piezas de igual tamaño; resuelve los dos subproblemas recursivamente y combina ambas soluciones en una solución general, en tiempo cuadrático para la división inicial y recombinación final.

* Propiedad: si $T(n)$ es el tiempo de ejecución de un algoritmo de este tipo, entonces $T(n)$ satisface la siguiente relación de recurrencia (5.6):
	* Para alguna constante $c$, $T(n) \leq 2 T(n/2) + cn^{2}$ cuando $n > 2$.
	* $T(2) \leq c$.
* En un nivel arbitrario $j$ hay $2^j$ subproblemas de tamaño $n/2^{j}$, por lo que el tiempo total está acotado por $cn^{2}/2^{j}$. Sumando sobre todos los niveles de recursión obtenemos la cota $T(n) \leq cn^{2} \sum\limits^{\log_{2} n-1}_{j = 0} \left(\frac{1}{2^{j}}\right) \leq 2cn^{2} = O(n^2)$.

### Contando inversiones

##### El problema

* Dada una secuencia de $n$ números $a_{1}, \dots, a_{n}$ distintos, queremos definir una medida de qué tan lejos está la lista de un orden ascendiente; el valor de la medida debe ser $0$ si $a_{1} < \dots < a_{n}$ y debería aumentar si los números están revueltos.
* Una forma natural de cuantificar esta noción es contando la cantidad de inversiones. Dos índices $i < j$ forman una _inversión_ si $a_{i} > a_{j}$, es decir que los elementos están fuera de orden. Buscamos determinar la cantidad de inversiones en la secuencia $a_{1}, \dots, a_{n}$.

##### Diseñando y analizando el algoritmo

* La idea básica es seguir la estrategia de dividir y conquistar. Fijamos $m = \lceil n/2 \rceil$ y dividimos la lista en dos piezas $a_{1}, \dots, a_{m}$ y $a_{m+1}, \dots, a_{n}$. Primero contamos la cantidad de inversiones en cada mitad por separado y luego contamos la cantidad de inversiones $(a_{i}, a_{j})$ donde los dos números pertenecen a mitades distintas, en tiempo $O(n)$. Haremos que el algoritmo ordene recursivamente los números en ambas mitades para hacer la combinación más fácil.
* La rutina crucial es $\texttt{Merge-and-Count}$. Tenemos la primer y segunda mitad ordenadas recursivamente en las listas $A$ y $B$, y la cantidad de inversiones en cada una. Queremos producir una lista ordenada única $C$ de su unión, mientras contamos la cantidad de pares $(a, b)$ con $a \in A$, $b \in B$ y $a > b$.
* Esta rutina recorre las listas $A$ y $B$ removiendo elementos del frente y agregándolos a la lista ordenada $C$. En un paso dado tenemos un puntero $\texttt{Current}$ apuntando a cada lista mostrando la posición actual. Si los punteros apuntan a $a_{i}$ y $b_{j}$, comparamos los elementos y quitamos el más pequeño de su lista, y lo agregamos al final de la lista $C$.
* Cada vez que un elemento $a_{i}$ es añadido a la lista $C$ no se encuentran nuevas inversiones, ya que $a_{i}$ es más pequeño que los elementos en $B$ y viene antes que todos ellos. Por otro lado, si $b_{j}$ es añadido a la lista $C$, entonces es más pequeño que todos los elementos restantes de $A$ y viene después de todos ellos, por lo que incrementamos el número de inversiones por la cantidad de elementos restantes de $A$. En tiempo constante consideramos una gran crantidad de inversiones.
* ![[figura5_5.png]]

* Algoritmo: Merge-and-Count
	* ![[mergeandcount.png]] ![[mergeandcount2.png]]

* Cada iteración del bucle $\texttt{while}$ toma tiempo constante y en cada una agregamos un elemento a la salida que no se vuelve a usar. Por lo tanto el tiempo total de ejecución de esta rutina es $O(n)$ (como máximo la suma de los largos de las listas $A$ y $B$).

* Algoritmo: $\texttt{Sort-and-Count}$
	* ![[sortandcount.png]]
	* ![[slide_sortandcount.png]]

* Propiedad: correctitud y tiempo de ejecución de $\texttt{Sort-and-Count}$ (5.7)
	* El algoritmo $\texttt{Sort-and-Count}$ ordena correctamente la entrada y cuenta la cantidad de inversiones en tiempo $O(n \log n)$ para una lista de $n$ elementos.

### Encontrando el par de puntos más cercano

* Problema:
	* Dados $n$ puntos en el plano, encontrar el par más cercano.
	* Sea $P = \{p_{1}, \dots, p_{n}\}$ el conjunto de puntos donde $p_{i}$ tiene coordenadas $(x_{i}, y_{i})$ y para dos puntos $p_{i}, p_{j} \in P$ utilizamos $d(p_{i}, p_{j})$ para denotar la distancia euclidea estándar entre los dos puntos. Nuestro objetivo es encontrar un par $p_{i}, p_{j}$ que minimice $d(p_{i}, p_{j})$.
	* Todos los puntos tienen coordenadas distintas.

##### Diseñando el algoritmo

* La estrategia a utilizar es divice y vencerás sobre el conjunto $P$: primero se encuentran los pares más cercanos en la mitad izquierda de $P$, en la mitad derecha de $P$ y se combinan las soluciones en tiempo lineal.
* El primer nivel de la recursión se define así, y se extiende análogamente para el resto de los niveles.
* Ordenamos todos los puntos de $P$ primero en orden ascendente según la coordenada $x$ y luego segundo la coordenada $y$, produciendo las listas $P_{x}$ y $P_{y}$ junto con un registro de la posición de cada punto en ambas listas.
* Sea $Q$ el conjunto de los primeros $\lceil n/2 \rceil$ puntos de la lista $P_{x}$ (la mitad izquierda) y $R$ el conjunto de los últimos $\lfloor n/2 \rfloor$ puntos de la la lista $P_{x}$ (la mitad derecha). 
* En una pasada sobre cada lista $P_{x}$ y $P_{y}$ en tiempo $O(n)$ creamos las siguientes cuatro listas: $Q_{x}$ que consiste en los puntos de $Q$ en orden ascendente según la coordenada $x$; $Q_{y}$ que consiste en los puntos de $Q$ en orden ascendente según la coordenada $y$ y listas análogas para $R_x$ y $R_{y}$.
* Determinamos recursivamente un par más cercano en $Q$ con acceso a las listas $Q_{x}$ y $Q_{y}$. Sean  $q_{0}^{*}$ y $q_{1}^{*}$ el par de puntos más cercano de $Q$. De forma similar determinamos un par de puntos más cercanos en $R$ obteniendo $r_{0}^{*}$ y $r_{1}^{*}$.
* Sea $\delta$ el mínimo de $d(q_{0}^{*}, q_{1}^{*})$ y $d(r_{0}^{*}, r_{1}^{*})$. La pregunta es si existen puntos $q \in Q$ y $r \in R$ tales que $d(q, r) < \delta$. Sea $x^{*}$ la coordenada $x$ del punto en el extremo derecho de $Q$ y sea $L$ la línea vertical de ecuación $x = x^{*}$. Esta línea separa $Q$ y $R$.

* Propiedad: (5.8)
	* Si existen $q \in Q$ y $r \in R$ tal que $d(q, r) < \delta$, entonces tanto $q$ como $r$ están a una distancia $\delta$ de $L$.

* Para encontrar $q$ y $r$ cercanos, restringimos la búsqueda a la banda de puntos en $P$ a distancia $\delta$ de $L$. Sea $S \subseteq P$ este conjunto y $S_{y}$ los puntos de $S$ en orden ascendente según la coordenada $y$. En una sola pasada sobre la lista $P_{y}$ construimos $S_{y}$ en tiempo $O(n)$.

* Propiedad: (5.9)
	* Existen $q \in Q$ y $r \in R$ tales que $d(q, r) < \delta$ si y solo sí existen $s, s' \in S$ tales que $d(s, s') < \delta$.

* Propiedad: (5.10)
	* Si $s, s' \in S$ cumplen que $d(s, s') < \delta$ entonces $s$ y $s'$ están a 15 posiciones de cada uno en la lista ordenada $S_{y}$.

* Para concluir el algoritmo se recorre $S_{y}$ y para cada $s \in S_{y}$ se calcula su distancia a cada uno de los siguientes 15 puntos en $S_{y}$. Habiendo hecho esto podemos reportar dos cosas: el par de puntos más cercano en $S$ si su distancia es menor a $\delta$ o la conclusión de que ningún par de puntos en $S$ están a distancia $\delta$ uno del otro. En el primer caso el par encontrado es el par más cercano de $P$, en el segundo el par más cercano es el encontrado por los llamados recursivos.

* Algoritmo: 
	* ![[closestpair1.png]]![[closestpair2.png]]

* Propiedad: correctitud y tiempo de ejecución (5.11, 5.12)
	* El algoritmo determina correctamente el par de puntos más cercano de $P$ en tiempo $O(n \log n)$.

### Multiplicación de enteros

* Problema:
	* Multiplicar dos números enteros $x$ e $y$.
	* Un algoritmo básico de escuela consiste en calcular un producto parcial multiplicando cada dígito de $y$ por $x$ y sumar todos los productos parciales.

* Escribimos $x = x_{1}\cdot2^{n/2} + x_{0}$. $x_{1}$ son los $n/2$ bits de alto orden y $x_{0}$ corresponde a los $n/2$ bits de bajo orden. De forma similar escribimos $y = y_{1}\cdot 2^{n/2} + y_{0}$.
* Tenemos que:
	* $xy = (x_{1}\cdot2^{n/2} + x_{0})(y_{1}\cdot 2^{n/2} + y_{0})$ $= x_{1} y_{1}\cdot 2^{n} + (x_{1}y_{0} + x_{0}y_{1})\cdot 2^{n/2} + x_{0} y_{0}$
	* Esta ecuación nos da cuatro instancias para resolver la multiplicación. Es un primer candidato para dividir y conquistar: calcular recursivamente el resultado para las cuatro instancias de $n/2$ bits y combinarlas como en la ecuación.
* Combinar la solución requiere una cantidad constante de sumas de números de $O(n)$ bits lo que toma tiempo $O(n)$. El tiempo total $T(n)$ está acotado por la recurrencia: $T(n) \leq 4T(n/2) + cn$ para alguna constante $c$.
* Llevamos la recurrencia al caso $q = 3$ en la clase de recurrencias de (5.3). La solución es $T(n) \leq O(n^{\log_{2} q}) = O(n^{1.59})$. El producto $(x_{1} + x_{0})(y_{1} + y_{0}) = x_{1}y_{1} + x_{1}y_{0} + x_{0}y_{1} + x_{0}y_{0}$ contiene los cuatro subproductos sumados. Si determinamos $x_{1}y_{1}$ y $x_{0}y_{0}$ recursivamente, obtenemos el término medio restando $x_{1}y_{1}$ y $x_{0}y_{0}$ de $(x_{1} + x_{0})(y_{1} + y_{0})$.

* Algoritmo:
	* ![[recursivemultiply.png]]

* Propiedad: tiempo de ejecución de $\texttt{Recursive-Multiply}$ (5.13)
	* El tiempo de ejecución de $\texttt{Recursive-Multiply}$ sobre dos factores de $n$ bits es $O(n^{\log_{2} 3}) = O(n^{1.59})$.

#### Complejidad de algoritmos de ordenamiento

