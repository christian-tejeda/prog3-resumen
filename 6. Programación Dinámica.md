# Programación Dinámica

* Nos concentramos en una técnica de diseño más potente y sutil, la _programación dinámica_. La idea básica se basa en la intuición de divide y vencerás, y esencialmente es opuesta la idea a la estrategia ávida: se explora el espacio de todas las posibles soluciones implícitamente, descomponiendo el problema cuidadosamente en una serie de subproblemas, y luego se construyen soluciones correctas a subproblemas más grandes.

## Planificación de intervalos ponderados: un procedimiento recursivo

* Problema: una versión más general del problema de planificación de intervalos, donde cada intervalo tiene un cierto _valor_, _costo_ o _peso_ asociado y queremos tener un conjunto de valor máximo.

#### Diseñando un algoritmo recursivo

* Tenemos $n$ pedidos etiquetados $1, \dots, n$, con cada pedido $i$ especificando un tiempo de comienzo $s_{i}$ y un tiempo de finalización $f_{i}$. Cada intervalo $i$ tiene un _valor_ o _peso_ $v_{i}$ asociado.
* Dos intervalos son compatibles si no se solapan en el tiempo.
* El objetivo del problema actual es seleccionar un conjunto $S \subseteq \{1, \dots, n\}$ de intervalos mutuamente compatibles que maximice la suma de los valores de los intervalos seleccionados: $\sum\limits_{i \in S} v_{i}$.
* Suponemos que los pedidos están ordenados de forma no decreciente según el tiempo de finalización: $f_{1} \leq f_{2} \leq \dots \leq f_{n}$.
* Decimos que un pedido $i$ viene _antes_ de un pedido $j$ si $i < j$.
* Definimos $p(j)$ para un intervalo $j$ como el índice $i$ más grande tal que $i < j$ y los intervalos $i$ y $j$ son disjuntos, es decir el índice $i$ que termina antes de que comienze $j$.  Ejemplo: ![[figura6_2.png|400]]
* Consideremos una solución óptima $\mathcal{O}$. Hay algo obvio que podemos decir sobre $\mathcal{O}$: el pedido $n$ pertenece o no a $\mathcal{O}$. Si $n \in \mathcal{O}$, entonces claramente ningún intervalo con índice estrictamente entre $p(n)$ y $n$ puede pertenecer a $\mathcal{O}$, ya que todos los intervalos entre $p(n), \dots, n-1$ se solapan con $n$. Además, $\mathcal{O}$ debe incluir una solución óptima al problema que consiste de los pedidos $\{1, \dots, p(n)\}$; de otra forma podríamos reemplazar la elección de pedidos en $\mathcal{O}$ con una mejor, sin riesgo de solapar al pedido $n$.
* Por otro lado, si $n \notin \mathcal{O}$, entonces $\mathcal{O}$ simplemente es igual a la solución óptima al problema que consiste en los pedidos $\{1, \dots, n - 1\}$ por un razonamiento análogo: como $\mathcal{O}$ no incluye a $n$, podemos reemplazar esa elección por otra mejor.
* Sea $\mathcal{O}_{j}$ la solución óptima para cualquier valor de $j$ entre $1$ y $n$, compuesta por los pedidos $\{1, \dots, j\}$ y sea $\textrm{OPT} (j)$ el valor de esta solución. ($\textrm{OPT}(0) = 0$ para el caso del conjunto vacío). La solución óptima que buscamos es $\mathcal{O}_{n}$ con el valor $\textrm{OPT}(n)$.
* Se pueden dar dos casos: si $j \in \mathcal{O}_{j}$ entonces $\textrm{OPT}(j) = v_{j} + \textrm{OPT}(p(j))$. O $j \notin \mathcal{O}_{j}$, en cuyo caso $\textrm{OPT}(j) = \textrm{OPT}(j - 1)$.

* Propiedad: caracterización de la solución óptima (6.1, 6.2)
	* $\textrm{OPT}(j) = \max(v_{j} + \textrm{OPT}(p(j)), \textrm{OPT}(j - 1))$.
	* El pedido $j$ pertenece a una solución óptima en el conjunto $\{1, \dots, j\}$ _sii_ $v_{j} + \textrm{OPT}(p(j)) \geq \textrm{OPT}(j - 1)$.

* Algoritmo: $\texttt{Compute-Opt}$
	* ![[wintsched_computeopt.png]]

* Propiedad: correctitud del algoritmo (6.3)
	* $\texttt{Compute-Opt}(j)$ calcula correctamente $\textrm{OPT}(j)$ para cada $j = 1, \dots, n$.

* Desafortunadamente si implementásemos el algoritmo $\texttt{Compute-Opt}$ tal como está escrito tomaría tiempo exponencial para la ejecución en el peor caso ya que el árbol de recursión crece muy rápido.

#### Memoizando la recursión

* Una observación fundamental es que el algoritmo recursivo $\texttt{Compute-Opt}$ solo resuelve realmente $n + 1$ subproblemas distintos. El tiempo exponencial se debe a la espectacular redundancia en el número de llamados que se hacen.
* Podemos eliminar la redundancia guardando el valor $\texttt{Compute-Opt}$ en un lugar accesible globalmente la primera vez que lo calculamos y usar el valor precalculado en su lugar en futuros llamados recursivos. Esta técnica de guardar valores ya calculados se llama _memoización_.
* Implementamos esta estrategia en el procedimiento $\texttt{M-Compute-Opt}$ que hace uso de un arreglo $M[0 \dots n]$; $M[j]$ comienza vacío y contendrá el valor de $\texttt{Compute-Opt}(j)$ cuando sea calculado. Para determinar $\textrm{OPT}(n)$ llamamos a $\texttt{M-Compute-Opt}(n)$.

* Algoritmo: $\texttt{M-Compute-Opt}$
	* ![[wintsched_mcomputeopt_1.png]] ![[wintsched_mcomputeopt_2.png]]
	* ![[slide_wintsched.png|600]]

#### Analizando la versión memoizada

* Propiedad: tiempo de ejecución de $\texttt{M-Compute-Opt}(n)$ (6.4) 
	* El tiempo de ejecución de $\texttt{M-Compute-Opt}(n)$ es $O(n)$ asumiendo que los intervalos están ordenados según su tiempo de finalización.
	* ![[slide_wintsched2.png|600]]

#### Calculando una solución además de su valor

* Hasta ahora solo calculamos el _valor_ de una solución óptima; presumiblemente queremos un conjunto óptimo y completo de intervalos también.
* Hacemos uso de la propiedad (6.2) para obtener un procedimiento simple que rastrea hacia atrás sobre el arreglo $M$ para hallar el conjunto de intervalos en una solución óptima.

* Algoritmo: $\texttt{Find-Solution}$
	* ![[wintsched_findsolution.png]]
	* ![[slide_findsolution.png|600]]

* Propiedad: tiempo de ejecución de $\texttt{Find-Solution}$ (6.5)
	* Dado el arreglo $M$ de los valores óptimos en los subproblemas, $\texttt{Find-Solution}$ retorna una solución óptima en tiempo $O(n)$.

### Principios de la programación dinámica: memoización o iteración sobre subproblemas

* Primero desarrollamos una solución en tiempo polinomial para el problema de planificación de intervalos ponderados diseñando primero un algoritmo recursivo de tiempo exponencial y convirtiéndolo con memoización a uno más eficiente.
* Reformulamos el algoritmo a una forma equivalente más explícita que captura la esencia de la programación dinámica y que servirá de plantilla para los algoritmos de secciones siguientes.

#### Diseñando el algoritmo

* La clave del algoritmo eficiente es el arreglo $M$ que utiliza (6.1) para obtener los valores en función de los anteriores. Una vez que está construido, el problema está resuelto: $M[n]$ contiene el valor de la solución óptima en la instancia completa y $\texttt{Find-Solution}$ se puede usar para rastraer hacia atrás sobre $M$ eficientemente y devolver una solución óptima.
* Podemos calcular las entradas de $M$ con un algoritmo iterativo en vez de memoizar la recursión. Comenzamos con $M[0] = 0$ e incrementamos $j$, cada vez que haya que determinar $M[j]$ la respuesta está provista por (6.1).

* Algoritmo: $\texttt{Iterative-Compute-Opt}$
	* ![[itercomputeopt.png]]

* Propiedad:
	* El tiempo de ejecución de $\texttt{Iterative-Compute-Opt}$ es $O(n)$.

#### Un esquema básico de la programación dinámica

* Para desarrollar un algoritmo basado en la programación dinámica se necesita una colección de subproblemas derivados del problema original que satisfaga ciertas propiedades básicas.
	1. Hay una cantidad polinomial de subproblemas.
	2. La solución al problema original puede ser calculada fácilmente a partir de las soluciones a los subproblemas.
	3. Hay un orden natural sobre los subproblemas, desde el más chico al más grande, junto con una recurrencia fácil de calcular que permita determinar la solución a un subproblema a partir de una cantidad de soluciones de subproblemas más pequeños.

## Mínimos cuadrados segmentados: elecciones múltiples

#### El problema

* Tenemos datos en forma de un conjunto $P$ de $n$ puntos en el plano $(x_{1}, y_{1}), \dots, (x_{n}, y_{n})$ y suponemos que $x_{1} < \dots < x_{n}$. $p_{i}$ denota el punto $(x_{i}, y_{i})$.
* Dada una línea $L = ax + b$ decimos que el _error_ de $L$ con respecto a $P$ es la suma de los cuadrados de las distancias de los puntos en $P$: $\textrm{Error}(L, P) = \sum\limits_{i=1}^{n}(y_{i}-ax_{i}-b)^{2}$
* Un objetivo natural es encontrar la línea con el mínimo error; sucede que una solución cerrada se puede derivar utilizando cálculo.

* Intentamos formular un nuevo problema: antes que buscar una sola línea que se ajuste, podemos colocar una cantidad arbitraria de lineas sobre los puntos y buscamos un conjunto de líneas que minimice el error. Una formulación que nos permite ajustar los puntos con la menor cantidad de líneas posibles: _mínimos cuadrados segmentados_.
* Dada una secuencia de puntos, queremos identificar unos pocos en la secuencia en los cuales ocurre un cambio discreto, en este caso un cambio de una aproximación lineal a otra.
* Primero debemos particionar $P$ en segmentos. Cada _segmento_ es un subconjunto de $P$ que representa un conjunto contiguo de coordenadas $x$, es decir un conjunto de la forma $\{p_{i}, \dots, p_{j}\}$ para algún par de índices $i < j$.
* Para cada segmento $S$ de la partición de $P$ calculamos la línea que minimiza el error respecto a los puntos de $S$.
* La _penalización_ de una partición se define como la suma de los siguientes términos:
	1. La cantidad de segmentos en los que particionamos $P$ por un multiplicador  fijo $C > 0$.
	2. Por cada segmento, el valor del error de la línea óptima sobre ese segmento.
* El objetivo del problema de mínimos cuadrados segmentados es encontrar una partición de penalización mínima. Hay una cantidad exponencial de posibles particiones de $P$ y usamos programación dinámica para encontrar una partición de penalización mínima en tiempo polinomial en $n$.

#### Diseñando el algoritmo

* La siguiente observación resulta muy útil: el último punto $p_{n}$ pertenece a un solo segmento en la partición óptima que comienza en el punto $p_{i}$. Si conocemos la identidad del último segmento $p_{i}, \dots, p_{n}$, podemos dejar de considerar esos puntos y resolver recursivamente el problema en los puntos $p_{1}, \dots, p_{i-1}$ restantes. ![[figura6_9.png]]
* Sea $\textrm{OPT}(i)$ la solución óptima para los puntos $p_{1}, \dots, p_{i}$ y sea $e_{i,j}$ el mínimo error de cualquier línea respecto a $p_{i}, \dots, p_{j}$. $\textrm{OPT}(0) = 0$ como caso borde.

* Propiedad: cálculo de la solución óptima (6.6, 6.7)
	* Si el último segmento de la solución es $p_{i}, \dots, p_{n}$ entonces el valor de la solución óptima es $\textrm{OPT}(n) = e_{i,n} + C + \textrm{OPT}(i - 1)$.
	* Para el subproblema sobre los puntos $p_{1}, \dots, p_{j}$: $\textrm{OPT}(j) = \min\limits_{1 \leq i \leq j} (e_{i,j} + C + \textrm{OPT}(i - 1))$ y el segmento $p_{i}, \dots, p_{j}$ se usa en la solución óptima para el subproblema _sii_ el mínimo se obtiene usando el índice $i$.

* Algoritmo: $\texttt{Segmented-Least-Squares}$
	* ![[segmentedleastsquares.png]]
	* ![[slides_segmentedleastsquares.png|600]]

* Algoritmo: $\texttt{Find-Segments}$
	* ![[findsegments.png]]

* Propiedad: tiempo de ejecución de $\texttt{Segmented-Least-Squares}$
	* El tiempo de ejecución del algoritmo es $O(n^{3})$ para el cálculo de $e_{i,j}$ y $O(n^{2})$ si se optimiza el cálculo de $e_{i,j}$.

### _Subset sums_ y _knapsacks_: agregando una variable

##### El problema

* Tenemos una sola máquina para procesar trabajos, y tenemos un conjunto de pedidos $\{1, \dots, n\}$. Solo podemos utilizar este recurso desde el tiempo $0$ hasta $W$. Cada trabajo toma tiempo o peso $w_{i}$ para ser procesado.
* El objetivo es elegir un subconjunto $S$ de los pedidos tal que $\sum\limits_{i \in S} w_{i} \leq W$ y que la suma sea lo más grande posible.
* Este es el _subset sum problem_. Una versión general es el _knapsack problem_ donde cada pedido $i$ tiene tanto un valor $v_{i}$ como un peso $w_{i}$ asociado. El objetivo es elegir un subconjunto de máximo valor posible y peso total sin exceder $W$.

* Asumimos que $W$ es entero y todos los pedidos tienen valores y pesos enteros. Tenemos un subproblema para cada $i = 0, 1, \dots, n$ y cada entero $0 \leq w_{i} \leq W$. Notamos como $\textrm{OPT}(i, w)$ el valor de la solución óptima que usa un subcojunto de $\{1, \dots, i\}$ con peso máximo permitido $w$: $\textrm{OPT}(i, w) = \max\limits_{S} \sum\limits_{j \in S} w_{j}$. El valor que buscamos es $\textrm{OPT}(n, W)$.
* Sea $\mathcal{O}$ la solución óptima para el problema original. Si $n \notin \mathcal{O}$, entonces $\textrm{OPT}(n, W) = \textrm{OPT}(n - 1, W)$ ya que podemos ignorar el ítem $n$.
* Si $n \in \mathcal{O}$, entonces $\textrm{OPT}(n, W) = w_{n} + \textrm{OPT}(n - 1, W - w_{n})$ ya que ahora buscamos utilizar la capacidad restante $W - w_{n}$ de forma óptima sobre los ítems $1, 2, \dots, n - 1$.
* Cuando el $n$-ésimo ítem es demasiado grande, es decir que $W < w_{n}$ entonces $\textrm{OPT}(n, W) = \textrm{OPT}(n - 1, W)$. De otro modo obtenemos la solución óptima sobre los $n$ pedidos tomando la mejor de las dos opciones.

* Propiedad: recurrencia de _subset sums_ (6.8)
	* Si $w < w_{i}$ entonces $\textrm{OPT}(i, w) = \textrm{OPT}(i - 1, w)$.
	* De lo contrario: $\textrm{OPT}(i, w) = \max(\textrm{OPT}(i - 1, w), w_{i} + \textrm{OPT}(i - 1, w - w_{i}))$.

* Algoritmo: $\texttt{Subset-Sum}$
	* ![[subsetsum.png]]

##### Analizando el algoritmo

* Creamos la tabla de soluciones $M$ calculando cada entrada $M[i, w]$ en tiempo $O(1)$ utilizando los valores previos. Por lo tanto el tiempo de ejecución total es proporcional a la cantidad de entradas en la tabla.
* ![[figura6_11.png]]

* Propiedad: tiempo de ejecución de $\texttt{Subset-Sum}$ (6.9)
	* El algoritmo $\texttt{Subset-Sum}(n, W)$ calcula correctamente el valor óptimo del problema y ejecuta en tiempo $O(nW)$.

* Propiedad: obtener el conjunto óptimo (6.10)
	* Dada una tabla $M$ de valores óptimops para los subproblemas, el conjunto óptimo $S$ se puede hallar en tiempo $O(n)$.

##### Extensión: el _knapsack problem_

* Cada ítem $i$ tiene un peso $w_{i}$ y un valor $v_{i}$. El objetivo es encontrar un subconjunto $S$ con valor $\sum\limits_{i \in S} v_{i}$ máximo y sujeto a la restricción de que el peso total no exceda $W$: $\sum\limits_{i \in S} w_{i} \leq W$
* Consideramos una solución óptima $\mathcal{O}$ e identificamos dos casos. Si $n \notin \mathcal{O}$, entonces $\textrm{OPT}(n, W) = \textrm{OPT}(n - 1, W)$.
* Si $n \in \mathcal{O}$, entonces $\textrm{OPT}(n, W) = v_{n} + \textrm{OPT}(n - 1, W - w_{n})$.

* Propiedad: solución óptima y tiempo de ejecución extendido (6.11, 6.12)
	* Si $w < w_{i}$, entonces $\textrm{OPT}(i, w) = \textrm{OPT}(i - 1, w)$.
	* De lo contrario: $\textrm{OPT}(i, w) = \max(\textrm{OPT}(i - 1, w), v_{i} + \textrm{OPT}(i - 1, w - w_{i}))$
	* El _knapsack problem_ puede ser resuelto en tiempo $O(nW)$.

#### Estructura secundaria ARN: programación dinámica sobre intervalos

* El ADN de doble cadena se une mediante el emparejamiento de bases complementarias. Cada cadena de ADN se puee ver como una cadena de _bases_, donde cada base se toma del conjunto $\{A, C, G, T\}$. Las bases $A$ y $T$, $C$ y $G$ van juntas.
* El ARN de cadena simple, al no tener una segunda cadena, se une a sí misma formando pares de bases consigo mismo a través de un proceso llamado _estructura secundaria_.
* Una molécula de ARN de cadena simple se puede ver como una secuencia de $n$ símbolos (bases) tomados del alfabeto $\{A, C, G, U\}$. Sea $B = b_{1} \dots b_{n}$ con $b_{i} \in \{A, C, G, U\}$.
* Pedimos que $A$ se una con $U$, y $C$ se una con $G$. Cada base debe unirse con como máximo otra base. En otras palabras el conjunto de pares de bases forma un emparejamiento.
* Una estructura secundaria sobre $B$ es un conjunto de pares $S = \{(i, j)\}$ donde  $i, j \in \{1, \dots, n\}$ que satisface las siguientes condiciones:
	1. Los extremos de cada par en $S$ están separados por al menos cuadro bases; si $(i, j) \in S$ entonces $i < j - 4$. (_Sin giros bruscos_).
	2. Los elementos de cualquier par en $S$ consisten de $\{A, U\}$ o $\{C, G\}$.
	3. $S$ es un emparejamiento: ninguna base aparece en más de un par.
	4. Si $(i, j)$ y $(k, l)$ son dos pares $S$, entonces no se puede dar que $i < k < j < l$. (Condición de no cruzamiento).
* Figura que ilustra las condiciones: ![[figura6_14.png]]

* Queremos un algoritmo eficiente que toma una molécula de ARN de cadena simple $B = b_{1} \dots b_{n}$ y determina una estructura secundaria $S$ con la cantidad máxima posible de pares de bases.

##### Diseñando y analizando el algoritmo

* Sea $\textrm{OPT}(i, j)$ la cantidad máxima de pares de bases en una estructura secundaria $b_{i} b_{i+1} \dots b_{j}$. La condición de _sin giros bruscos_ nos permite inicializar $\textrm{OPT}(i, j) = 0$ cuando $i \geq j - 4$.
* En la estructura secundaria óptima $b_{i} b_{i+1} \dots b_{j}$ tenemos las siguientes alternativas: $j$ no está en un par o $j$ está emparejado con $t$ para algún $t < j - 4$.
* En el primer caso, $\textrm{OPT}(i, j) = \textrm{OPT}(i, j - 1)$. En el segundo caso, consideramos los dos subproblemas $\textrm{OPT}(i, t - 1)$ y $\textrm{OPT}(t + 1, j - 1)$. La condición de no cruzamiento aisla ambos subproblemas entre sí.

* Propiedad: cálculo del valor óptimo para la estructura secundaria (6.13)
	* $$\textrm{OPT}(i, j) = \max(\textrm{OPT}(i, j - 1),\max(1 + \textrm{OPT}(i, t - 1) + \textrm{OPT}(t + 1, j - 1)))$$ donde el máximo se toma sobre $t$ tal que $b_{t}$ y $b_{j}$ son un par de bases permitido bajo las condiciones (1) y (2) de la estructura secundaria.

* Algoritmo: cálculo de la estructura secundaria óptima
	* ![[secondarystructure.png]]

* Propiedad: tiempo de ejecución del algoritmo de estructura secundaria
	* Hay $O(n^{2})$ subproblemas donde cada uno se resuelve en tiempo $O(n)$, el tiempo total de ejecución es $O(n^{3})$.

### Alineación de secuencias

##### El problema

* ¿Cómo definimos la similaridad entre dos palabras o cadenas?
* Tenemos las palabras _ocurrencia_ y _ocorencia_. 
* Si alineamos las palabras letra a letra, colocamos un _guión_ (-) donde falta una letra para dejarlas alineadas (hueco) y vemos un desajuste en las letras que no coinciden (error).
	1. ocurrencia
	2. ocor-encia
* Buscamos un modelo en donde la similaridad entre dos palabras esté dada por el número de huecos y errores cuando las alineamos de esta forma.

* El _genoma_ de un organismo se divide en moléculas de ADN gigantes conocidas como _cromosomas_, un dispositivo químico de almacenamiento unidimensional.
* Se puede pensar como una tira sobre el alfabeto $\{A, C, G, T\}$.
* Si analizamos dos cepas de bacterias $X$ e $Y$ y sabemos que cierta subcadena en el ADN de $X$ se relaciona a cierta toxina. Si encontramos una subcadena "similar" en el ADN de $Y$ podemos hipotetizar que esa porción de ADN corresponde a un tipo de toxina similar.

* Tenemos dos cadenas $X$ e $Y$ donde $X$ consiste de la secuencia de símbolos $x_{1} \dots x_{m}$ e $Y$ consiste de la secuencia de símbolos $y_{1} \dots y_{n}$. Los conjuntos $\{1, \dots, m\}$ y $\{1, \dots, n\}$ representan las diferentes posiciones en las cadenas $X$ e $Y$. 
* Consideramos un emparejamiento $M$ de esos conjuntos y decimos que es una _alineación_ si no hay pares "cruzados": si $(i, j), (i', j') \in M$ y $i < i'$ entonces $j < j'$.
* Nuestra definición de similaridad se basará en encontrar una alineación óptima entre $X$ e $Y$ según el siguiente criterio. Dado $M$ una alineación entre $X$ e $Y$:
	*  Hay un parámetro $\delta > 0$ que define el _costo del hueco_. Para cada posición de $X$ o $Y$ que no esté emparejado en $M$ (es un hueco), pagamos un costo $\delta$.
	* Para cada par de letras $p$ y $q$ del alfabeto, hay una _costo de error_ $\alpha_{pq}$ por alinear $p$ y $q$. Entonces para cada $(i, j) \in M$ pagamos el costo de error correspondiente $\alpha_{x_{i} y_{j}}$ por alinear $x_{i}$ e $y_{j}$. $\alpha_{pp} = 0$ cuando se alinea $p$ con una copia de sí misma.
	* El _costo_ de $M$ es la suma de los costos de huecos y errores, y buscamos una alineación de costo mínimo.

##### Diseñando el algoritmo

* Propiedad: caracterización de las alineaciones (6.14, 6.15)
	* Sea $M$ una alineación cualquiera de $X$ e $Y$. Si $(m, n) \notin M$ entonces la $m$-ésima posición de $X$ o la $n$-ésima posición de $Y$ no está emparejada en $M$.
	* En una alineación óptima $M$, al menos una de las siguientes propiedades se cumple:
		1. $(m, n) \in M$
		2. La $m$-ésima posición de $X$ no está emparejada
		3. La $n$-ésima posición de $Y$ no está emparejada

* Sea $\textrm{OPT}(i, j)$ el costo mínimo de una alineación entre $x_{1} \dots x_{i}$ e $y_{1} \dots y_{j}$.
* Si el caso (1) de (6.15) se cumple, pagamos el costo $\alpha_{x_{m} y_{n}}$ y alineamos tan bien como sea posible a $x_{1}\dots x_{m - 1}$ e $y_{1}\dots y_{n-1}$: obtenemos $\textrm{OPT}(m, n) = \alpha_{x_{m} y_{n}} + \textrm{OPT}(m - 1, n - 1)$.
* Si el caso (2) se cumple pagamos un costo $\delta$ ya que la $m$-ésima posición de $X$ no está emparejada y obtenemos $\textrm{OPT}(m, n) = \delta + \textrm{OPT}(m - 1, n)$.
* Si el caso (3) se cumple tenemos de forma análoga $\textrm{OPT}(m, n) = \delta + \textrm{OPT}(m, n -1)$.

* Propiedad: recurrencia de alineación de costo mínimo (6.16)
	* La alineación de costo mínimo satisface la siguiente recurrencia para $i, j \geq 1$: $$\textrm{OPT}(i, j) = \min[\alpha_{x_{i} y_{j}}+ \textrm{OPT}(i - 1, j - 1), \delta + \textrm{OPT}(i - 1, j), \delta + \textrm{OPT}(i, j - 1)]$$
	* $(i, j)$ está en una alineación óptima $M$ para este subproblema _sii_ el mínimo se alcanza por el primero de esos valores.

* Algoritmo: cálculo de la alineación óptima
	* ![[sequencealignment.png]]

* Lema: tiempo de ejecución de $\texttt{Alignment}$
	* El tiempo de ejecución del algoritmo es $O(mn)$. El tamaño de $A$ es $O(mn)$ y en el peor se toma tiempo constante en cada entrada del arreglo $A$.

### Caminos más cortos en un grafo

##### El problema

* Sea $G = (V, E)$ un grafo dirigido. Cada arista $(i, j) \in E$ tiene un peso o costo asociado $c_{ij}$. Consideramos un problema más complejo donde los costos pueden ser negativos.
* Dado un grafo $G$ con sus costos, decidir si $G$ tiene un ciclo negativo, es decir un ciclo dirigido $C$ tal que $\sum\limits_{ij \in C} c_{ij} < 0$.
* Si el grafo no tiene ciclos negativos, encontrar un camino $P$ desde un nodo origen $s$ a un nodo destino $t$ con costo total mínimo: $\sum\limits_{ij \in P} c_{ij}$.
* Propiedad: caracterización de caminos cortos con costos negativos (6.22)
	* Si $G$ no tiene ciclos negativos, entonces hay un camino más corto desde $s$ hacia $t$ que es simple (no repite nodos) y tiene como máximo $n - 1$ aristas.

* Sea $\textrm{OPT}(i, v)$ el costo mínimo de un camino $v-t$ que utiliza como máximo $i$ aristas. Por (6.22), el problema es calcular $\textrm{OPT}(n - 1, s)$.
* Sea $P$ un camino óptimo que representa a $\textrm{OPT}(i, v)$.
	* Si el camino $P$ usa como máximo $i - 1$ aristas, entonces $\textrm{OPT}(i, v) = \textrm{OPT}(i - 1, v)$.
	* Si el camino $P$ utiliza $i$ aristas y la primer arista es $(v, w)$, entonces $\textrm{OPT}(i, v) = c_{vw} + \textrm{OPT}(i - 1, w)$.

* Propiedad: recurrencia para caminos cortos (6.23)
	* Si $i > 0$, entonces $\textrm{OPT}(i, v) = \min(\textrm{OPT}(i - 1, v), \min\limits_{w \in V}(\textrm{OPT}(i - 1, w) + c_{vw}))$.

* Algoritmo: cálculo de $\textrm{OPT}(n - 1, s)$
	* ![[shortestpath_dp.png]]
	* ![[slide_shortestpath_dp.png|600]]

* Propiedad: (6.24)
	* El algoritmo $\texttt{Shortest-Path}$ calcula correctamente el costo mínimo de un camino $s-t$ de cualquier grafo sin ciclos negativos y tiene tiempo de ejecución $O(n^{3})$.